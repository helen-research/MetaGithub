{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import numpy as np\n",
    "from sklearn.externals import joblib\n",
    "from github import Github\n",
    "import gensim\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = []\n",
    "# File from: wget http://data.gharchive.org/2018-07-01-15.json.gz\n",
    "with gzip.open('2018-07-01-15.json.gz') as f:\n",
    "    for line in f:\n",
    "        events.append(json.loads(line.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter((event['type'] for event in events));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "issues = [event for event in events if event['type'] == 'IssuesEvent']\n",
    "comments = [event for event in events if event['type'] == 'IssueCommentEvent']\n",
    "CreateEvents = [event for event in events if event['type'] == 'CreateEvent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "actors=[]\n",
    "#Hold a list of all the usernames and repo names\n",
    "#This is needed in order to pull README.md files\n",
    "names=[]\n",
    "## pull all of the createEvents in order to get UserID and Repo name for \n",
    "# calls to get Readme Content\n",
    "CreateEvents = [event for event in events if event['type'] == 'CreateEvent']\n",
    "for event in range(1,len(CreateEvents)):\n",
    "    thisEvent= CreateEvents[event]\n",
    "    actors.append(thisEvent['repo'])\n",
    "    for name in range(0,len(actors)):\n",
    "        thisName=actors[name]\n",
    "        names.append(thisName['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## gets rid of doubles:\n",
    "names = np.array(names)\n",
    "names = np.unique(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This code will hopefully create a sparsely encoded corpus of readMe files:\n",
    "def iter_documents(repo_names):\n",
    "    \"\"\"\n",
    "    Generator: iterate over all relevant documents, yielding one\n",
    "    document (=list of utf8 tokens) at a time.\n",
    "    \"\"\"\n",
    "    # find all readMe files:\n",
    "    for readMeFile in range(0,len(repo_names)):\n",
    "        g = Github(\"6efe0a63859ff9f96f2e1b829db3525b33d2f533 \")\n",
    "        try:\n",
    "            thisRepo=g.get_repo(repo_names[readMeFile])\n",
    "            readMe=thisRepo.get_contents('README.md')\n",
    "            readMe = readMe.decoded_content\n",
    "        except:\n",
    "            pass\n",
    "            readMe =\"No README.md found.\"\n",
    "        # break document into utf8 tokens\n",
    "        yield gensim.utils.tokenize(readMe, lower=True, errors='ignore')\n",
    "\n",
    "class ReadMeTxtCorpus(object):\n",
    "    \"\"\"\n",
    "    Iterable: on each iteration, return bag-of-words vectors,\n",
    "    one vector for each document.\n",
    " \n",
    "    Process one document at a time using generators, never\n",
    "    load the entire corpus into RAM.\n",
    " \n",
    "    \"\"\"\n",
    "    def __init__(self, repo_names):\n",
    "        self.repo_names = repo_names\n",
    "        # create dictionary = mapping for documents => sparse vectors\n",
    "        self.dictionary = gensim.corpora.Dictionary(iter_documents(repo_names))\n",
    " \n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Again, __iter__ is a generator => TxtSubdirsCorpus is a streamed iterable.\n",
    "        \"\"\"\n",
    "        for tokens in iter_documents(self.repo_names):\n",
    "            # transform tokens (strings) into a sparse vector, one at a time\n",
    "            yield self.dictionary.doc2bow(tokens)\n",
    " \n",
    "# that's it! the streamed corpus of sparse vectors is ready\n",
    "corpus = ReadMeTxtCorpus(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Github(\"6efe0a63859ff9f96f2e1b829db3525b33d2f533 \")\n",
    "forks = []\n",
    "watchers = []\n",
    "stargazers = []\n",
    "subscribers = []\n",
    "for repoName in range(0,len(names)):\n",
    "    thisRepo=g.get_repo(names[repoName])\n",
    "    try:\n",
    "        forks.append(thisRepo.forks_count)\n",
    "    except:\n",
    "        forks.append(0)\n",
    "    try:\n",
    "        watchers.append(thisRepo.watchers_count)\n",
    "    except:\n",
    "        watchers.append(0)\n",
    "    try:\n",
    "        stargazers.append(thisRepo.stargazers_count)\n",
    "    except:\n",
    "        stargazers.append(0)\n",
    "    try:\n",
    "        subscribers.append(thisRepo.subscribers_count)\n",
    "    except:\n",
    "        subscribers.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a pandas dataframe holding all the variables for tracking the \n",
    "# 'popularity' measures of the repos:\n",
    "readMesAndPopularity = pd.DataFrame(\n",
    "    {'username/repoName': names,\n",
    "     'forks': forks,\n",
    "     'stargazers': stargazers,\n",
    "     'subscribers': subscribers,\n",
    "     'ReadMes': corpus\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['readMeFiles.pkl']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(readMesAndPopularity, \"readMeFiles.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3529"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readMeFiles = joblib.load(\"readMeFiles.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
